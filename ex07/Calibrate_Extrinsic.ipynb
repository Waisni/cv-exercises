{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration of a End-of-Arm Camera\n",
    "\n",
    "This exercise will describe how to calibrate a end-of-arm camera using marker-board detection.\n",
    "The excercise is divided into the following steps.\n",
    "\n",
    "1. Load recorded images.\n",
    "2. Show marker detection results.\n",
    "3. Solve for the calibration.\n",
    "4. Analysis of results.\n",
    "\n",
    "First lets have a look at the data.\n",
    "\n",
    "Note: if the interactive viewer does not work you may have to restart the notebook with `%matplotlib widget` instead of `%matplotlib notebook`\n",
    "\n",
    "Note: As of 2023-12, interactive matplotlib only works with older notebook versions so downgrade your versions with\n",
    "`pip install \"notebook<7\"`\n",
    "Start it in the command line via` jupyter notebook --port xxxx`.\n",
    "Use the vs-code pop up to open jupyter in your local server.\n",
    "\n",
    "For some of the visualization you will need open3d, install with `pip install open3d`:\n",
    "If you cannot get it to run you may ignore it, it is not necessary for the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='calibration_ls.png', width=1000)  # image adapted from torsteinmyhre.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise want to do eye-in-hand extrinsic calibration. In this setup, a camera (eye) is mounted at end of a robot arm. The end of the robot arm is called Tool Center Point (TCP) because this is where tools are mounted. We would like to find the transformation `T_cam_tcp`. We do this calibration by moving the robot and taking several pictures of the marker board.\n",
    "\n",
    "In the setup, the marker board, and the base of the robot are static, this means that `T_cam_tcp` as well as `T_robot_marker` are fixed. For each view recorded the robot arm is in a different location, which changes the position of the marker board as seen by the camera. This means that `T_tcp_robot` and `T_cam_marker` change for each view. Calibration works by finding a loop in these poses, and solving for `T_cam_tcp`. This can be done in several ways, in this exercise we present two methods, a simple least-squares optimization and the peak-martin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import io\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from scipy.spatial.transform import Rotation\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_exception(e, with_traceback=False) -> str:\n",
    "    error_str, error_name = str(e), type(e).__name__\n",
    "    if error_str == \"\":\n",
    "        out_str = error_name\n",
    "    else:\n",
    "        out_str = f\"{error_name}: {error_str}\"\n",
    "\n",
    "    if not with_traceback:\n",
    "        return out_str\n",
    "\n",
    "try:\n",
    "    import open3d as o3d\n",
    "except Exception as e:\n",
    "    o3d = None\n",
    "    print(format_exception(e))\n",
    "    print(\"Try installing open3d with: pip install open3d\")\n",
    "    print(\"If you cannot get it to run you may ignore it, it is not necessary for the exercise.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code will download the data. Alternatively you can find it in `/project/.../shared-data/marker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = Path(\".\")\n",
    "if not (target_path / \"marker\").is_dir():\n",
    "    url = \"https://lmb.informatik.uni-freiburg.de/people/argusm/marker.tar\"\n",
    "    print(f\"Downloading {url}\")\n",
    "    res = requests.get(url, timeout=30)\n",
    "    data = res.content\n",
    "    data_io = io.BytesIO(data)\n",
    "    with tarfile.TarFile(fileobj=data_io, mode='r') as tar:\n",
    "        tar.extractall(path=target_path.as_posix())\n",
    "else:\n",
    "    print(f\"Folder exists: {target_path.as_posix()}/marker\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recorded Images\n",
    "\n",
    "This cell defines a data class that loads images and data from files.\n",
    "\n",
    "Please complete the `get_projection_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewLoader:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        assert os.path.isdir(base_path)\n",
    "        files = sorted(os.listdir(self.base_path))\n",
    "        files = [f for f in files if (f.startswith(\"rgb_\") and f.endswith(\".png\"))]\n",
    "        self.max_idx = int(files[-1].replace(\"rgb_\", \"\").replace(\".png\", \"\"))\n",
    "        print(f\"Loaded {self.max_idx+1} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_idx + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_rgbdp(idx)\n",
    "\n",
    "    def get_info(self):\n",
    "        # read given parameters from info.json\n",
    "        info_path = os.path.join(self.base_path, 'info.json')\n",
    "        with open(info_path, \"rb\") as f_obj:\n",
    "            info  = json.load(f_obj)\n",
    "        return info\n",
    "\n",
    "    def get_intrinsics(self):\n",
    "        # get intrintic camera matrix\n",
    "        info = self.get_info()\n",
    "        calib = info[\"camera\"][\"calibration\"]\n",
    "        return calib\n",
    "\n",
    "    def get_K(self):\n",
    "        calib = self.get_intrinsics()\n",
    "        cam_intrinsic = np.eye(3)\n",
    "        cam_intrinsic[0, 0] = calib[\"fx\"]\n",
    "        cam_intrinsic[1, 1] = calib[\"fy\"]\n",
    "        cam_intrinsic[0, 2] = calib[\"ppx\"]\n",
    "        cam_intrinsic[1, 2] = calib[\"ppy\"]\n",
    "        return cam_intrinsic\n",
    "\n",
    "    def get_robot_pose(self, idx, return_dict=False):\n",
    "        # read robot pose, convert to 4x4 homogeneous format\n",
    "        pose_file = os.path.join(self.base_path, \"pose_{0:04d}.json\".format(idx) )\n",
    "        with open(pose_file,\"rb\") as f_obj:\n",
    "            pose = json.load(f_obj)\n",
    "        pose_m = np.eye(4)\n",
    "        pose_m[:3, :3] = Rotation.from_euler(\n",
    "            \"xyz\", [pose[x] for x in ['rot_x', 'rot_y', 'rot_z']]).as_matrix()\n",
    "        pose_m[:3, 3] = [pose[x] for x in ['x', 'y', 'z']]\n",
    "        if return_dict:\n",
    "            return pose_m, pose\n",
    "        else:\n",
    "            return pose_m\n",
    "\n",
    "    # read RGB image\n",
    "    def get_rgb_file(self, idx):\n",
    "        rgb_file = os.path.join(self.base_path, \"rgb_{0:04d}.png\".format(idx) )\n",
    "        return rgb_file\n",
    "\n",
    "    # read depth image\n",
    "    def get_depth_file(self, idx):\n",
    "        depth_file = os.path.join(self.base_path, \"depth_{0:04d}.png\".format(idx) )\n",
    "        return depth_file\n",
    "\n",
    "    # get RGB, scaled-depth, robot pose\n",
    "    def get_rgbdp(self, idx):\n",
    "        rgb_file = self.get_rgb_file(idx)\n",
    "        depth_file = self.get_depth_file(idx)\n",
    "\n",
    "        pose_m, pose_d = self.get_robot_pose(idx, True)\n",
    "        # depth\n",
    "        depth_scaling = pose_d[\"depth_scaling\"]\n",
    "        rgb  = np.asarray(Image.open(rgb_file))\n",
    "        depth = np.asarray(Image.open(depth_file), dtype=np.float32) * depth_scaling\n",
    "        return rgb, depth, pose_m\n",
    "\n",
    "    def get_cam_pose(self, idx, marker_dir=\"pose_marker_one\"):\n",
    "        # get camera pose in 4x4 homogeneous format\n",
    "        marker_dir = os.path.join(self.base_path, marker_dir)\n",
    "        fn  = \"{0:08d}.json\".format(idx)\n",
    "        pose_fn = os.path.join(marker_dir, fn)\n",
    "        with open(pose_fn, \"r\") as fo:\n",
    "            T = np.array(json.load(fo))\n",
    "        return T\n",
    "\n",
    "    def get_projection_matrix(self):\n",
    "        # returns a 3x4 projection matrix using the intrinsics\n",
    "        # it projects a 2d point in in homogeneous coordinates\n",
    "        # to a 3d point in homogeneous coordinates\n",
    "        # assuming the camera frame and world frame are aligned\n",
    "\n",
    "        # START TODO #################\n",
    "        # intr = self.get_intrinsics()\n",
    "        # ...\n",
    "        raise NotImplementedError\n",
    "        # END TODO ###################\n",
    "        assert cam_mat.shape == (3, 4)\n",
    "        return cam_mat\n",
    "\n",
    "\n",
    "    def project(self, X):\n",
    "        # project an (homogeneous) cartesian coordinate into the camera frame.\n",
    "        if X.shape[0] == 3:\n",
    "            if len(X.shape) == 1:\n",
    "                X = np.append(X, 1)\n",
    "            else:\n",
    "                X = np.concatenate([X, np.ones((1, X.shape[1]))], axis=0)\n",
    "\n",
    "        x = self.get_projection_matrix() @ X\n",
    "        result = np.round(x[0:2] / x[2]).astype(int)\n",
    "        width, height = self.get_intrinsics()['width'], self.get_intrinsics()['height']\n",
    "        if not (0 <= result[0] < width and 0 <= result[1] < height):\n",
    "            log.warning(\"Projected point outside of image bounds\")\n",
    "        return result[0], result[1]\n",
    "\n",
    "vl = ViewLoader(base_path=\"marker\")\n",
    "print(\"camera calibration:\")\n",
    "camera_calibration = vl.get_K()\n",
    "K = np.array(camera_calibration)\n",
    "print(K.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout, interact\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image, depth, pose = vl.get_rgbdp(0)\n",
    "line = ax.imshow(np.asarray(image))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    line.set_data(np.asarray(image))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_w = widgets.IntSlider(min=0, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Show Marker Detection Results.\n",
    "\n",
    "To simplify things marker detection has already been run. Next we want to verify its results.\n",
    "Do this by drawing a coordinate frame into each image for which we have detection results.\n",
    "The coordinate frame should have axis lengths of 10cm, with x=red, y=green, and z=blue.\n",
    "This can be done using `PIL.ImageDraw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def show_marker_pose(image, T_cam_marker):\n",
    "    \"\"\"\n",
    "    draw the coordinate frame into each image for which we have detection results\n",
    "    Arguments:\n",
    "        image: image as numpy.ndarray\n",
    "        T_cam_marker: shape (4, 4), transform from marker into camera\n",
    "            x_cam = T_cam_marker @ x_marker\n",
    "    Returns:\n",
    "        im: image (should be PIL.Image.Image)\n",
    "    \"\"\"\n",
    "    print(T_cam_marker)\n",
    "    # START TODO #################\n",
    "    # using PIL.ImageDraw\n",
    "    # 1. Define 4 points in 3d homogeneous coordinates: <x, y, z, center>\n",
    "    #    (The center of the coordinate system and a point for each of 3 axes.)\n",
    "    # 2. Transform the <x, y, z, center> coordinates into the camera frame\n",
    "    # 3. Project the homogeneous coordinates <cam_x, cam_y, cam_z, center>\n",
    "    #    to the camera image (euclidean)\n",
    "    # 4. Draw one line for each axis\n",
    "    raise NotImplementedError\n",
    "    # END TODO ###################\n",
    "\n",
    "    # type(im) should be PIL.Image.Image\n",
    "    return im\n",
    "\n",
    "image, depth, robot_pose = vl.get_rgbdp(1)\n",
    "T_cam_marker = vl.get_cam_pose(1)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image_m = show_marker_pose(image, T_cam_marker)\n",
    "line = ax.imshow(np.asarray(image_m))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    try:\n",
    "        T_cam_marker = vl.get_cam_pose(w)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No pose estimation.\")\n",
    "        line.set_data(np.asarray(image))\n",
    "        return\n",
    "    image_m = show_marker_pose(image, T_cam_marker)\n",
    "    line.set_data(np.asarray(image_m))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_w = widgets.IntSlider(min=0, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tcp_marker_lists(m_dir=\"pose_marker_one\"):\n",
    "    T_robot_tcp_list = []\n",
    "    T_cam_marker_list = []\n",
    "    for i in range(len(vl)):\n",
    "        try:\n",
    "            robot_pose = vl.get_robot_pose(i)\n",
    "            cam_pose = vl.get_cam_pose(i, marker_dir=m_dir)\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            continue\n",
    "        T_robot_tcp_list.append(robot_pose)\n",
    "        T_cam_marker_list.append(cam_pose)\n",
    "\n",
    "    return np.array(T_robot_tcp_list), np.array(T_cam_marker_list)\n",
    "\n",
    "if o3d is None:\n",
    "    print(f\"Skip o3d as it was not imported correctly\")\n",
    "else:\n",
    "    T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "    # create one big frame to show the absolute zero point of the plot\n",
    "    mesh_frames = []\n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n",
    "    mesh_frames.append(mesh_frame)\n",
    "\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        # create coordinate frames around the zero point and transform them\n",
    "        # to visualize the transforms\n",
    "\n",
    "        # # T_robot_tcp shows where the tool is relative to the robot\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "        mesh_frame.transform(T_robot_tcp)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "\n",
    "        # # you can also view the robot base relative to the tool\n",
    "        # T_tcp_robot = np.linalg.inv(T_robot_tcp)\n",
    "        # mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "        # mesh_frame.transform(T_tcp_robot)\n",
    "        # mesh_frames.append(mesh_frame)\n",
    "\n",
    "        # # shows where the cam is relative to the marker\n",
    "        T_marker_cam = np.linalg.inv(T_cam_marker)\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.20)\n",
    "        mesh_frame.transform(T_marker_cam)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "\n",
    "        # # shows where the marker is relative to the cam\n",
    "        # mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.20)\n",
    "        # mesh_frame.transform(T_cam_marker)\n",
    "        # mesh_frames.append(mesh_frame)\n",
    "\n",
    "    # Draw the geometries\n",
    "    o3d.visualization.draw_geometries(mesh_frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solve for the Calibration\n",
    "\n",
    "We will try to find the calibration using two different methods, via least squares optimization and using the park martin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_matrix(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: is our optimization target, a vector of shape (9,).\n",
    "            [0:3] position transform of T_cam_tcp\n",
    "            [3:6] rotation transform of T_cam_tcp\n",
    "                (angles to rotate around xyz axes)\n",
    "            [6:9] position transform of T_robot_marker\n",
    "\n",
    "    Returns:\n",
    "        transformation matrix shape (4, 4)\n",
    "\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.from_euler.html\n",
    "    \"\"\"\n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 3] = x[0:3]\n",
    "    mat[:3, :3] = Rotation.from_euler('xyz', x[3:6]).as_matrix()\n",
    "    return mat\n",
    "\n",
    "def pprint(arr):\n",
    "    return np.array2string(arr.round(5), separator=', ')\n",
    "\n",
    "def matrix_to_pos_orn(mat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mat: 4x4 homogeneous transformation\n",
    "\n",
    "    Returns:\n",
    "        position: np.array of shape (3,),\n",
    "        orientation: np.array of shape (4,) -> quaternion xyzw\n",
    "\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.as_quat.html\n",
    "    \"\"\"\n",
    "    orn = Rotation.from_matrix(mat[:3, :3]).as_quat()\n",
    "    pos = mat[:3, 3]\n",
    "    return pos, orn\n",
    "\n",
    "def calculate_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list, inliers=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        T_tcp_cam: Transform from camera to tool center shape (4,4)\n",
    "        T_robot_tcp_list: List of robot poses i.e. transform from tool center\n",
    "            point to robot base, shape (N, 4, 4)\n",
    "        T_cam_marker_list: List of marker measurements i.e. transform from\n",
    "            marker to camera, shape (N, 4, 4)\n",
    "    Returns:\n",
    "        scalar error for each entry\n",
    "    \"\"\"\n",
    "    T_robot_marker_list = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_robot_marker = T_robot_tcp @ T_tcp_cam @ T_cam_marker\n",
    "        T_robot_marker_list.append(T_robot_marker)\n",
    "\n",
    "    poses = np.array(T_robot_marker_list)[:, :3, 3]\n",
    "    if inliers is None:\n",
    "        mean_pose = np.mean(poses, axis=0)\n",
    "    else:\n",
    "        mean_pose = np.mean(poses[inliers], axis=0)\n",
    "    err = np.sum((poses - mean_pose)**2, axis=1)\n",
    "    return err\n",
    "\n",
    "def pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list):\n",
    "    \"\"\"\n",
    "    returns position tuple for each entry\n",
    "\n",
    "    Args:\n",
    "        T_tcp_cam: Transform from camera to tool center shape (4,4)\n",
    "        T_robot_tcp_list: List of robot poses i.e. transform from tool center\n",
    "            point to robot base, shape (N, 4, 4)\n",
    "        T_cam_marker_list: List of marker measurements i.e. transform from\n",
    "            marker to camera, shape (N, 4, 4)\n",
    "    Returns:\n",
    "        error: cartesian error (N, 3)\n",
    "    \"\"\"\n",
    "    T_robot_marker_list = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_robot_marker = T_robot_tcp @ T_tcp_cam @ T_cam_marker\n",
    "        T_robot_marker_list.append(T_robot_marker)\n",
    "\n",
    "    poses = np.array(T_robot_marker_list)[:, :3, 3]\n",
    "    err = poses - np.mean(poses, axis=0)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Squares Optimization\n",
    "\n",
    "`T_robot_marker` and `T_cam_tcp` are fixed but unknown, so these will be used as variables that have to be optimized.\n",
    "From these values compute the predicted `T_cam_marker_pred` and compare it to `T_cam_marker_obs`.\n",
    "We can do the comparison based only on position values, so `T_robot_marker` only uses position components.\n",
    "Optimize the function using `scipy.optimize.least_squares`.\n",
    "\n",
    "`T_cam_marker = T_cam_tcp @ T_tcp_robot @ T_robot_marker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares, minimize\n",
    "\n",
    "def compute_residuals_gripper_cam(x, T_robot_tcp_list, T_cam_marker_list):\n",
    "    \"\"\"\n",
    "    Calculate predicted positional transform from marker to camera using\n",
    "    T_cam_tcp, T_robot_marker and T_tcp_robot.\n",
    "    Compare these predicted values with the observed positional transofrm\n",
    "    T_cam_marker to calculate and returns the residuals\n",
    "\n",
    "    Args:\n",
    "        x: is our optimization target, a vector of shape (9,).\n",
    "            [0:3] position transform of T_cam_tcp\n",
    "            [3:6] rotation transform of T_cam_tcp\n",
    "                (angles to rotate around xyz axes)\n",
    "            [6:9] position transform of T_robot_marker\n",
    "        T_robot_tcp_list: List of robot poses i.e. transform from tool center\n",
    "            point to robot base, each entry shape (4, 4)\n",
    "        T_cam_marker_list: List of marker measurements i.e. transform from\n",
    "            marker to camera, each entry shape (4, 4)\n",
    "\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.from_euler.html\n",
    "    \"\"\"\n",
    "    T_robot_marker = np.array([*x[6:], 1])  # shape (4, )\n",
    "    T_cam_tcp = vec_to_matrix(x) # shape (4, 4)\n",
    "\n",
    "    # START TODO #################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ###################\n",
    "\n",
    "    # len(residuals) will be 144= 48 samples * 3 (x,y,z), for least-squares optimization\n",
    "    return residuals\n",
    "\n",
    "def calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list):\n",
    "    # use scipy least squares to optimize the above function and return the calibration\n",
    "    # START TODO #################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ###################\n",
    "\n",
    "    assert T_calib.shape == (4, 4)\n",
    "    return T_calib\n",
    "\n",
    "T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "T_calib = calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to slighty improve the result using differential evolution\n",
    "# nothing required here.\n",
    "import scipy\n",
    "def calibrate_gripper_cam_de(T_robot_tcp_list, T_cam_marker_list):\n",
    "    x0 = np.array([0, 0, 0, 0, 0, 0, 0, 0, -0.1])\n",
    "    result = least_squares(fun=compute_residuals_gripper_cam, x0=x0, method='lm',\n",
    "                           args=(T_robot_tcp_list, T_cam_marker_list))\n",
    "    T_calib = np.linalg.inv(vec_to_matrix(result.x))\n",
    "\n",
    "    x0 = result.x\n",
    "    bounds = [(x-.0001, x+.0001) for x in x0]\n",
    "    def func(x, *args):\n",
    "        T_cam_tcp = vec_to_matrix(x)\n",
    "        return calculate_error(T_cam_tcp, *args).mean()\n",
    "    result2 = scipy.optimize.differential_evolution(func=func, bounds=bounds,\n",
    "                           args=(T_robot_tcp_list, T_cam_marker_list), tol=1e-11)\n",
    "    T_calib = np.linalg.inv(vec_to_matrix(result2.x))\n",
    "    #print(result.x)\n",
    "    #print(bounds)\n",
    "    return T_calib\n",
    "\n",
    "T_calib = calibrate_gripper_cam_de(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Park-Martin Calibration\n",
    "\n",
    "Have a look here for the description of the Park-Martin method.\n",
    "\n",
    "https://www.torsteinmyhre.name/snippets/robcam_calibration.html\n",
    "\n",
    "We are trying to solve the following equation: `AX = XB`. The website shows how to create the lists `As`,`Bs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def log_func(R):\n",
    "    # Rotation matrix logarithm\n",
    "    theta = np.arccos((R[0,0] + R[1,1] + R[2,2] - 1.0)/2.0)\n",
    "    return np.array([R[2,1] - R[1,2], R[0,2] - R[2,0], R[1,0] - R[0,1]]) * theta / (2*np.sin(theta))\n",
    "\n",
    "def invsqrt(mat):\n",
    "    u,s,v = np.linalg.svd(mat)\n",
    "    return u.dot(np.diag(1.0/np.sqrt(s))).dot(v)\n",
    "\n",
    "def calibrate(A, B):\n",
    "    #transform pairs A_i, B_i\n",
    "    N = len(A)\n",
    "    M = np.zeros((3, 3))\n",
    "    for i in range(N):\n",
    "        Ra, Rb = A[i][0:3, 0:3], B[i][0:3, 0:3]\n",
    "        M += np.outer(log_func(Rb), log_func(Ra))\n",
    "\n",
    "    Rx = np.dot(invsqrt(np.dot(M.T, M)), M.T)\n",
    "\n",
    "    C = np.zeros((3*N, 3))\n",
    "    d = np.zeros((3*N, 1))\n",
    "    for i in range(N):\n",
    "        Ra, ta = A[i][0:3, 0:3], A[i][0:3, 3]\n",
    "        Rb, tb = B[i][0:3, 0:3], B[i][0:3, 3]\n",
    "        C[3*i:3*i+3, :] = np.eye(3) - Ra\n",
    "        d[3*i:3*i+3, 0] = ta - np.dot(Rx, tb)\n",
    "\n",
    "    tx = np.dot(np.linalg.inv(np.dot(C.T, C)), np.dot(C.T, d))\n",
    "    X = np.eye(4)\n",
    "    X[:3, :3] = Rx\n",
    "    X[:3, 3] = tx.flatten()\n",
    "    return X\n",
    "\n",
    "def calibrate_gripper_cam_peak_martin(T_robot_tcp_list, T_cam_marker_list):\n",
    "    ECs = []\n",
    "    for T_robot_tcp, t_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_tcp_robot = np.linalg.inv(T_robot_tcp)\n",
    "        ECs.append((T_tcp_robot, t_cam_marker))\n",
    "\n",
    "    As = []  # relative EEs\n",
    "    Bs = []  # relative cams\n",
    "    for pair in itertools.combinations(ECs, 2):\n",
    "        (e_1, c_1), (e_2, c_2) = pair\n",
    "        A = e_2 @ np.linalg.inv(e_1)\n",
    "        B = c_2 @ np.linalg.inv(c_1)\n",
    "        As.append(A)\n",
    "        Bs.append(B)\n",
    "\n",
    "        # symmetrize\n",
    "        A = e_1 @ np.linalg.inv(e_2)\n",
    "        B = c_1 @ np.linalg.inv(c_2)\n",
    "        As.append(A)\n",
    "        Bs.append(B)\n",
    "\n",
    "    X = calibrate(As, Bs)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis of results\n",
    "\n",
    "Lets have a look at the results of both methods. \n",
    "\n",
    "1. How much does each datapoint contribute to the overall error? How much does each cartesian dimension contribute to the overall error?\n",
    "2. Does the error correlate with distance from the camera?\n",
    "3. How do the results change if we remove high error datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "T_calib = calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list)\n",
    "\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))\n",
    "\n",
    "T_calib = calibrate_gripper_cam_peak_martin(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_pm = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_pm_s = np.sum(err_pm**2, axis=1)\n",
    "print(\"median error pm\", np.median(err_pm_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO #################\n",
    "# 1. plot errors for each datapoint and method\n",
    "# 2. plot errors for each datapoint, method and axis\n",
    "raise NotImplementedError\n",
    "# END TODO ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error-Depth Correlation\n",
    "\n",
    "Does the error correlate with distance from the camera?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO #################\n",
    "raise NotImplementedError\n",
    "# END TODO ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Remove High-Error Samples.\n",
    "\n",
    "How do the results change if we remove high error datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO #################\n",
    "raise NotImplementedError\n",
    "# END TODO ###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Complete\n",
    "\n",
    "The addtional material is provided as a quick introduction into Open3D. There is nothing to do there, run the code if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally lets plot errors as a funciton of T_robot_tcp positions\n",
    "# This can be done using the open3d create_coordinate_frame command.\n",
    "if o3d is None:\n",
    "    print(f\"Skip o3d as it was not imported correctly\")\n",
    "else:\n",
    "    mesh_frames = []\n",
    "    err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "    err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "    err_ls_sn = err_ls_s / np.sum(err_ls_s)\n",
    "    for T_robot_tcp, err in zip(T_robot_tcp_list, err_ls_sn):\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05+err)\n",
    "        mesh_frame.transform(T_robot_tcp)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "    o3d.visualization.draw_geometries(mesh_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Merged Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if o3d is None:\n",
    "    print(f\"Skip o3d as it was not imported correctly\")\n",
    "else:\n",
    "    first_rgb = Image.open(vl.get_rgb_file(1))\n",
    "    K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "    K_o3d.set_intrinsics(first_rgb.size[1], first_rgb.size[0],\n",
    "                         K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "    pcd_list = []\n",
    "    for i in range(len(vl)):\n",
    "        try:\n",
    "            rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "            depth_file = Image.open(vl.get_depth_file(i))\n",
    "            T_c = vl.get_cam_pose(i)\n",
    "            T_r = vl.get_robot_pose(i)\n",
    "            depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            continue\n",
    "\n",
    "        rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "        depth = o3d.geometry.Image(np.array(depth_file).astype(np.uint16))\n",
    "        rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(rgb, depth,\n",
    "                                              depth_scale=1.0/depth_scaling, depth_trunc=1.0,\n",
    "                                              convert_rgb_to_intensity=False)\n",
    "        pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)\n",
    "\n",
    "        #T_est = T_r @ T_calib\n",
    "        T_est = np.linalg.inv(T_c)\n",
    "        pcd.transform(T_est)\n",
    "        pcd_list.append(pcd)\n",
    "\n",
    "    # sum pointclouds for easier visualization\n",
    "    pcd_all = pcd_list[0]\n",
    "    for pcd_cur in pcd_list[1:]:\n",
    "        pcd_all += pcd_cur\n",
    "    o3d.visualization.draw_geometries([pcd_all])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40de0b164a2ead70a1213ee87ce739cfc5594d2111c42683eb8f5e0739ba5537"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}